---
layout: post
title: "Deep Learning 词汇中英文对照"
description: "Dictionary of Deep Learning"
keywords: "ML, DL, ANN, UFLDL"
category: 机器学习
tags: [ML, DL, ANN, UFLDL]
---

收集来自：[UFLDL Tutorial](http://ufldl.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B)

感谢UFLDL的翻译组~

时不时翻看一下Deep Learning相关词汇还是很有收获的，权当温故相关亦可。(百度不会拉黑我吧，哈哈)

#### 中英文对照

<!-- more -->

|(batch) gradient descent|（批量）梯度下降法|
|(overall) cost function|（整体）代价函数|
|Activation|激活值|
|Autoencoder|自编码器|
|Autoencoders|自编码算法|
|Backpropagation|反向传播|
|Backpropagation Algorithm|反向传播算法|
|Bayesian regularization method|贝叶斯规则化方法|
|Bernoulli random variable|伯努利随机变量|
|Contiguous Groups|连接区域|
|Convolution|卷积|
|Deep Networks|深度网络|
|Feedforward|前向输送|
|Fine tuning|微调|
|Fine-tuning|微调|
|First-order features|一阶特征|
|Full Connected Networks|全联通网络|
|Gaussian prior|高斯先验概率|
|Greedy layer-wise training|逐层贪婪训练法|
|Hadamard product|阿达马乘积|
|Hessian matrix|黑塞矩阵|
|Hierarchical grouping|层次型分组|
|Higher-order features|更高阶特征|
|IID|独立同分布|
|KL divergence|相对熵|
|Linear Decoders|线性解码器|
|Locally Connected Networks|部分联通网络|
|Logistic Regression|逻辑回归|
|MAP|极大后验估计|
|Newton's method|牛顿法|
|PCA|主元分析|
|Part-whole decomposition|部分-整体分解|
|Pool|池化|
|PreTrain|预训练|
|Principal Components Analysis|主成份分析|
|Principal Components Analysis (PCA)|主成分分析|
|Raw inputs|原始输入|
|Second-order features|二阶特征|
|Softmax Regression|Softmax回归|
|Sparse Autoencoder|稀疏编码|
|Sparsity|稀疏性|
|Stacked autoencoder|栈式自编码神经网络（可以考虑翻译为“多层自动编码机”或“多层自动编码神经网络”）|
|Stationary|固有特征|
|Visual Cortex|视觉皮层|
|activation|激活值（可以考虑翻译为“激励响应”或“响应”）|
|activation function|激励函数|
|active|激活|
|autoencoder|自编码器|
|average firing rate|平均激活率|
|average sum-of-squares error|均方差|
|backpropagation|反向传播算法|
|basis|基|
|batch gradient ascent|批量梯度上升法|
|bias term|偏置项|
|bias terms|偏置项|
|bias units|偏置项|
|binary classification|二元分类|
|class labels|类型标记|
|conjugate gradient|共轭梯度|
|cost function|代价函数|
|covariance matrix|协方差矩阵|
|decorrelation|去相关|
|deep learning|深度学习|
|deep networks|深层网络|
|deep neural networks|深度神经网络|
|derivative|导函数|
|diagonal|对角线|
|diffusion of gradients|梯度的弥散|
|dimensionality reduction|降维|
|eigenvalue|特征值|
|eigenvector|特征向量|
|error term|偏差项|
|example|样例|
|extract|提取|
|features|特征|
|feedforward neural network|前馈神经网络（参照Mitchell的《机器学习》的翻译）|
|feedforward pass|前馈传递|
|fine-tune|微调|
|fine-tuned|微调|
|forward pass|前向传播|
|forward propagation|正向传播|
|gradient|梯度|
|gradient descent|梯度下降|
|hidden (layer) units|隐藏层单元|
|hidden layer|隐含层|
|hidden unit|隐单元|
|hidden units|隐藏神经元|
|highly non-convex optimization problem|高度非凸的优化问题|
|hyperbolic tangent|双曲正切函数|
|hypothesis|估值函数/估计值|
|identity activation function|恒等激励函数|
|inactive|抑制|
|input layer|输入层|
|intensity|亮度|
|intercept term|截距项|
|learning rate|学习速率|
|linear activation function|线性激励函数|
|logistic regression|logistic回归|
|magnitude|幅值|
|maximum likelihood estimation|极大似然估计|
|mean|平均值|
|mean value|均值|
|multi-class classification|多元分类|
|neural network|神经网络|
|neural networks|神经网络|
|neuron|神经元|
|non-convex function|非凸函数|
|non-linear transformation|非线性变换|
|normalization|归一化|
|numerical roundoff errors|数值舍入误差|
|numerically checking|数值检验|
|numerically reliable|数值计算上稳定|
|object detection|物体检测|
|off-by-one error|缺位错误|
|output layer|输出层|
|over-fitting|过拟合|
|overall cost function|总体代价函数|
|part-whole decompositions|“部分-整体”的分解|
|parts of objects|目标的部件|
|penalty term|惩罚因子|
|pooling|池化|
|pre-training|预训练|
|redundant|冗余|
|reflection matrix|反射矩阵|
|regularization|正则化|
|regularization term|规则化项|
|represent compactly|简洁地表达|
|robust|鲁棒|
|self-taught learning|自学习方法|
|semi-supervised learning|半监督学习|
|sigmoid activation function|S型激励函数|
|significant digits|有效数字|
|singular value|奇异值|
|singular vector|奇异向量|
|smoothing|平滑|
|sorted in decreasing order|降序排列|
|sparse autoencoder|稀疏自编码器|
|sparsity parameter|稀疏性参数|
|sparsity penalty|稀疏惩罚|
|squared-error|方差|
|stationarity|平稳性|
|step-size|步长值|
|supervised learning|有监督学习|
|symmetric positive semi-definite matrix|对称半正定矩阵|
|symmetry breaking|对称失效|
|tanh function|tanh激励函数|
|the average activation|平均活跃度|
|the derivative checking method|梯度验证方法|
|the log likelihood|对数似然函数|
|the objective|目标函数|
|the pixel intensity value|像素灰度值|
|training examples|训练样本|
|translation invariant|平移不变性|
|unrolling|组合扩展|
|unsupervised feature learning|非监督特征学习|
|unsupervised learning|无监督学习|
|variance|方差|
|vectorization|向量化|
|vectorized implementation|向量化实现|
|weight|权重|
|weight decay|权重衰减|
|weighted average|加权平均值|
|whitening|白化|
|zero-mean|均值为零| 

#### 符号一览表

|符号|含义|
|$$x$$|训练样本的输入特征，$$x\epsilon \Re ^{n}$$.|
|$$y$$|输出值/目标值.这里$$y$$可以是向量.在autoencoder中，$$y=x$$.|
|$$(x^{(i)},y^{(i)})$$|第$$i$$个训练样本|
|$$h_{W,b}(x)$$|输入为$$x$$时的似输出,其中包含参数$$W,b$$.该输出应当与目标值$$y$$具有相同的维数.|
|$$W_{ij}^{(l)}$$|连接第$$l$$层$$j$$单元和第$$l+1$$层$$i$$单元的参数.|
|$$b_{i}^{(l)}$$|第$$l+1$$层$$i$$单元的偏置项. 也可以看作是连接第$$l$$层偏置单元和第$$l+1$$层$$i$$单元的参数.|
|$$\theta $$|参数向量. 可以认为该向量是通过将参数$$W,b$$组合展开为一个长的列向量而得到. |
|$$a_i^{(l)}$$|网络中第$$l$$层$$i$$单元的激活（输出）值.另外，由于$$L_1$$层是输层，所以$$a^{(1)}_i=x_i$$. |
|$$f(\cdot )$$|激活函数. 本文中我们使用$$f(z) = \tanh(z)$$.|
|$$z^{(l)}_i$$|第$$l$$层$$i$$单元所有输入的加权和. 因此有$$a^{(l)}_i = f(z^{(l)}_i)$$.|
|$$\alpha$$|学习率|
|$$s_l$$|第$$l$$层的单元数目（不包含偏置单元）.|
|$$n_l$$|网络中的层数. 通常$$L_1$$层是输入层，$$L_{n_l}$$层是输出层. |
|$$\lambda$$|权重衰减系数.|
|$$\hat{x}$$|对于一个autoencoder，该符号表示其输出值；亦即输入值$$x$$的重构值. 与$$h_{W,b}(x)$$含义相同. |
|$$\rho$$|稀疏值，可以用它指定我们所需的稀疏程度|
|$$\hat\rho_i$$|（sparse autoencoder中）隐藏单元$$i$$的平均激活值.|
|$$\beta$$|（sparse autoencoder目标函数中）稀疏值惩罚项的权重.|


#### 小脚本

顺便附上整理词汇的python小脚本，感兴趣可以看一下。[LINK](https://github.com/shsfoolish/daily-code/blob/master/python/20150116/txt.md)








  