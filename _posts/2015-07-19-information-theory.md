---
layout: post
title: '熵'
description: "信息论 - 熵"
keywords: "NLP, Entropy"
category: 自然语言处理
tags: [NLP, Entropy]
---

[信息论](https://zh.wikipedia.org/wiki/%E4%BF%A1%E6%81%AF%E8%AE%BA)将信息的传递作为一种统计现象来考虑，给出了估算通信信道容量的方法。通常将“信息论之父”[香农](https://zh.wikipedia.org/wiki/%E5%85%8B%E5%8A%B3%E5%BE%B7%C2%B7%E9%A6%99%E5%86%9C)于1948年10月发表于《贝尔系统技术学报》上的论文《A Mathematical Theory of Communication》（通信的数学理论）作为现代信息论研究的开端。

信息论的基本内容的应用包括无损数据压缩（如ZIP文件）、有损数据压缩（如MP3和JPEG）、信道编码（如数字用户线路（DSL））。这个领域处在数学、统计学、计算机科学、物理学、神经科学和电机工程学的交叉点上。信息论对航海家深空探测任务的成败，光盘的发明，手机的可行性，互联网的发展，语言学和人类感知的研究，对黑洞的了解，和许多其他领域都影响深远。 

![]({{ site.qiniudn }}/images/2015071903.jpg)

<!-- more -->

#### 一、熵

熵（entropy）是信息论的基本概念。

如果$$X$$是一个离散型随机变量，取值空间为$$\mathbb{R}$$,其概率分布为$$p(x)=P(X=x),x\in \mathbb{R}$$.那么,$$X$$的熵$$H(X)$$定义为:

$$H(X)=-\sum_{x\in \mathbb{R}}p(x)log_{2}p(x)$$

其中,约定$$0log0=0$$. $$H(X)$$可以写成$$H(p)$$.由于上式中对数以2为底,该公式定义的熵的单位为二进制位(比特).通常将$$log_{2}p(x)$$写成$$logpp(x)$$

熵又称为自信息(self-information),可以视为描述一个随机变量的不确定性的数量.

它表示信源$$X$$每发一个符号(不论发什么符号)所提供的平均信息量.一个随机变量的熵越大,它的不确定性越大,那么,正确估计其值的可能性就越小.

#### 二、联合熵和条件熵

如果$$X,Y$$是一对离散型随机变量$$X,Y~p(x,y),X,Y$$的联合熵(join entropy)$$H(X,Y)$$定义为

$$H(X,Y)=-\sum_{x\in X}\sum_{y\in Y} p(x, y)logp(x,y)$$

联合熵实际上描述一对随机变量平均所需要的信息量.

给定随机变量$$X$$的情况下,随机变量Y的条件熵(conditional entropy)为:

$$H(X|Y)=-\sum_{x\in X}\sum_{y\in Y} p(x, y)logp(y|x)$$

将上两式子展开可得

$$H(X,Y)=H(X)+H(Y|X)$$

熵的连锁规则(chain rule for entropy)推广到一般情况,有

$$H(X_{1},X_{1}, ... X_{n})=H(X_1)+H(X_2 |X_3)+ ... + H(X_n | X_1 , ... , X_{n-1})$$

#### 三、互信息

根据熵的连锁规则,有

$$H(X,Y)=H(X)+H(Y|X)=H(Y)+H(X|Y)$$

因此,

$$H(X)-H(X|Y)=H(Y)-H(Y|X)$$

这个差叫做$$X$$和$$Y$$的互信息(mutual information,MI),记作$$I(X;Y)$$

$$I(X;Y)$$反映的是在知道了Y的值以后X的不确定性的减少量.可以理解为$$Y$$的值透露了多少关于$$X$$的信息量.

互信息和熵之间的关系如下图:

![]({{ site.qiniudn }}/images/2015071901.jpg)

$$H(X)=H(X)-H(X|X)=I(X;X)$$

上式一方面也说明了为什么熵又称为自信息,另一方面说明了两个完全相互依赖的变量之间的互信息并不是一个常量,而是取决于它们的熵.

互信息在NLP中的词汇聚类(word clustering)、汉语自动分词、词义消歧等问题的研究中具有重要用途.

#### 四、相对熵

相对熵(relative entropy)又称Kullback-Leible差异(Kullback-Leibler divergence),或简称KL距离,是称是相同事件空间里两个概率分布相对差距的.两个概率分布$$p(x)$$和$$q(x)$$的相对熵定义为

$$D(p\parallel q)=\sum_{x\in X}p(x)log \frac{p(x)}{q(x)}$$

该定义中约定$$0log(0/q)=0,plog(p/0)=\infty $$.表示成期望值为

$$D(p\parallel q)=E_p(log\frac{p(X)}{q(X)})$$

显然,当两个随机分布完全相同时,即$$p=q$$,其相对熵为0.当两个随机分布的差别增加时,其相对熵期望值也增大.

互信息实际上就是衡量一个联合分布与独立性差距多大的测度:

$$I(X;Y)=D(p(x,y)\parallel p(x)p(y))$$

#### 五、交叉熵

交叉熵的概念就是用来衡量估计模型与真实概率分布之间差异情况的。

如果一个随机变量$$X~p(x),q(x)$$为用于近似$$p(x)$$的概率分布，那么随机变量$$X$$和模型$$q$$之间的交叉熵(cross entropy)定义为：

$$H(X,q)=H(X)+D(p\parallel q) =-\sum_xp(x)logq(x)=E_p(log\frac 1 {q(x)})$$

由此，可以定义语言$$L=(X_i)~p(x)$$与其模型$$q$$的交叉熵为:

$$H(L,q)=-\lim_{x\rightarrow \infty}\frac{1}{n}\sum_{x_1^n}logq(x_1^n)$$

其中,$$x_1^n=x_1,x_2,...,x_n$$为$$L$$的语句,$$p(x_1^n)$$为L中$$x_1^n$$的概率,$$q(x_1^n)$$为模型$$q$$对$$x_1^n$$的概率估计.

至此, 仍然无法计算这个语言的交叉熵, 国为我们并不知道真实概率$$p(x_1^n)$$, 不过可以假设这种语言是"理想"的, 即$$n$$趋于无穷大时, 其全部"单词"的概率和为$$1$$. 也就是说, 根据信息论的定理: 假定语言$$L$$是稳态(stationary)遍历的(ergodic)随机过程, $$L$$与其模型$$q$$的交叉熵计算公式就变为

$$H(L,q)=-\lim_{x\rightarrow \infty}\frac{1}{n}logq(x_1^n)$$

由此, 可以根据模型$$q$$和一个含有大量数据的$$L$$的样本来计算交叉熵. 在设计模型$$q$$时, 目的是使交叉熵最小, 从而使模型最接近真实的概率分布$$p(x)$$. 一般地, 在$$n$$足够大时我们近似地彩如下计算方法: 

$$H(L, q)\approx \frac{1}{n}logq(x_1^n)$$

交叉熵与模型在测试语料中分配给每个单词的平均概率所表达的含义正好相反, 模型的交叉熵越小, 模型的表现越好.

#### 六、困惑度

在设计语言模型时, 通常用困惑度(perlexity)来代替交叉熵衡量语言模型的好坏. 给定语言L的样本$$l_1^n=l_1...l_n$$, $$L$$的困惑度$$PP_q$$定义为

$$PP_q=2^{H(L,q)}\approx 2^{-\frac {1}{n}logq(l_1^n)=[q(l_1^n)]^{-\frac {1}{n}}}$$

同样, 语言模型设计的任务就是寻找困惑度最小的模型, 使其最接近真实语言的情况.

在自然语言处理中, 语言模型 的困惑度通常是指语言模型对于测试数据的困惑度 



